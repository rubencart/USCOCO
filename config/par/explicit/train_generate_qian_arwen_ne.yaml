dataset: 'coco17'
captions: 'coco'

cfg_name: 'detr-qian-ne_l'
wandb_project_name: 'USCOCO'




rprecision:
 checkpoint: '/cw/working-rose/rubenc/MMMAli/output/2022_04_20_12_38_02_r-precision-cpn-nc_41/checkpoints/last.ckpt'








cuda: True
pin_memory: True
num_workers: 10
seed: 41
do_test: False
do_train: True
do_validate_during_training: True
batch_size: 128
val_batch_size: 128
nb_of_pos_bins: 15
lr_schedule: 'linear_with_warmup'
lr_schedule_warmup_epochs: 5
debug: False
debug_num_workers: 0
wandb_offline: False
deterministic: False
use_latent_tree_representations: True
latent_tree_val_h5: '/cw/working-frodo/rubenc/vpcfg-dev/output/pred_trees_2021_11_18_11_26_01_multi-gold_v2/val2017_pred_trees_wt.h5'
latent_tree_train_h5: '/cw/working-frodo/rubenc/vpcfg-dev/output/pred_trees_2021_11_18_11_26_01_multi-gold_v2/train2017_pred_trees_wt.h5'
latent_tree_absurd_h5: '/cw/working-frodo/rubenc/vpcfg-dev/output/pred_trees_2021_11_18_11_26_01_multi-gold_v2/absurd_pred_trees_wt.h5'
ground_truth_latent_trees: False
use_cpn: True
use_plm: True
train:
 gpus: -1
 precision: 32       # todo
 max_steps: -1
 gradient_clip_val: 3.0
text_encoder:
 weight_decay: 0.0
 lr: 0.00
 txt_enc_finetune: False
 txt_enc_pretrained: True

 text_encoder: plm

 hf_model_name_or_path: 'uclanlp/visualbert-vqa-coco-pre'
 hf_tokenizer_model_name_or_path: 'bert-base-uncased'
 huggingface_offline: True

 plm_add_structured_mask: False
model:
 predict_num_queries: True
 autoregressive: False
 max_target_positions: 96
 length_loss_coef: 0.1
 length_label_smoothing: 0.05
 no_pos_encoding: False
 use_additional_detr_encoder: True
 detr_encoder_for_length_only: True
 encoder_embed_dim: 768
detr:
 encoder_layers: 1
 dropout: 0.1
 dropout_bbox_embed: 0.1
 dropout_token_embed: 0.1
 weight_decay: 0.01
 noise_as_queries: False
 learnt_query_embeds: True
 query_noise_aggregate: 'sum'        # or 'concat'
 query_pos_first_layer_only: False
 probabilistic_bbox_predictions: False
 aux_loss: False
 decoder_embed_dim: 256
 lr: 0.0001
 label_smoothing: 0.05
 label_smoothing_pos: 0.05
 set_cost_class_coef: 1.0
 set_cost_bbox_coef: 1.0
 set_cost_giou_coef: 0.4
 set_cost_prob_bbox_coef: 0.0
 prob_bbox_loss_coef: 0.0
 bbox_loss_coef: 1.0
 giou_loss_coef: 1.0
 bbox_prop_loss_coef: 1.0
 bbox_rel_loss_coef: 1.0
 nobj_coef: 0.1
 condition_bbox_prediction_on_label: null  # probs , embed , logits , null , softmax_embed
 softmax_embed_training_argmax_inference: True
