dataset: 'coco17'
captions: 'coco'

cfg_name: 'gpt2-lg_attn50_probe'
wandb_project_name: 'MMMAliProbe'


cuda: True
pin_memory: True
seed: 45
do_train: True
do_validate_during_training: True
batch_size: 12
val_batch_size: 12
nb_of_pos_bins: 15
debug: False
debug_num_workers: 0
wandb_offline: False
deterministic: False
early_stop: 'val_sent_micro_tag_macro_f1'    # val_loss
early_stop_min_or_max: 'max'
early_stop_patience: 5
model_checkpoint_monitor_min_or_max: 'max'
model_checkpoint_monitor: 'val_sent_micro_tag_macro_f1'
use_cpn: True
filter_out_crowds: True
use_plm: False
use_tg: False
num_words: 1000
num_workers: 10
probe:
 batch_size: 50

 h5_path: '/cw/working-theoden/rubenc/MMMAli/output/2023_02_28_17_10_23_detr_s-43_gpt2-lg-attn05/test_run/probe_embeddings.h5'
 h5_index_path: '/cw/working-theoden/rubenc/MMMAli/output/2023_02_28_17_10_23_detr_s-43_gpt2-lg-attn05/test_run/id_to_h5_idx.json'
 learn_negative_constituents: True
 learn_tags: False
 current_layer: 0
 loop_over_layers: True
 lr_schedule_patience: 3
 start_at_layer: 5
train:
 gpus: -1
 max_epochs: 1000
 precision: 32
 gradient_clip_val: 3.0
text_encoder:
 weight_decay: 0.0
 lr: 0.00
 txt_enc_finetune: False
 txt_enc_pretrained: True

 text_encoder: 'huggingface'

 hf_model_name_or_path: 'gpt2-large'
 hf_tokenizer_model_name_or_path: 'gpt2-large'

 tg_right_branching: False
model:
 predict_num_queries: True
 autoregressive: False
 max_target_positions: 96
 length_loss_coef: 0.1
 length_label_smoothing: 0.05

 no_pos_encoding: True
 use_additional_detr_encoder: True
detr:
 dropout: 0.1
 dropout_bbox_embed: 0.1
 dropout_token_embed: 0.1
 weight_decay: 0.01
 noise_as_queries: False
 learnt_query_embeds: True
 query_noise_aggregate: 'sum'        # or 'concat'
 query_pos_first_layer_only: False
 probabilistic_bbox_predictions: False
 aux_loss: False

 lr: 0.0001
 label_smoothing: 0.05
 label_smoothing_pos: 0.05
 set_cost_class_coef: 1.0
 set_cost_bbox_coef: 1.0
 set_cost_giou_coef: 0.4
 set_cost_prob_bbox_coef: 0.0
 prob_bbox_loss_coef: 0.0
 struct_loss_coef: 0.25
 struct_loss_input: tree_pos_embs  # 'encoder_out'  # 'tree_pos_embs'
 nobj_coef: 0.1
 condition_bbox_prediction_on_label: null  # probs , embed , logits , null , softmax_embed
 softmax_embed_training_argmax_inference: True
